# WikiStreams 프로젝트 트러블슈팅 및 개선 기록

이 문서는 `wikiStreams` 프로젝트의 데이터 분석 스크립트를 개발하면서 마주친 문제점들과 이를 해결하기 위해 적용한 방안들을 기록한 것입니다.

## 1. 분석 스크립트의 정확도 개선

초기 분석 스크립트(`analyze_trends.py`)는단순히 편집 횟수만을 집계하여 실제 의미있는 트렌드를 파악하기 어려웠습니다. 결과가 시스템 활동이나 분류(Category) 편집에 의해 과도하게 부풀려지는 문제가 있었습니다.

### 문제점: 분석 결과의 노이즈 (Noise)

- **원인 1: 분류(Categorize) 활동 집계:** 문서에 카테고리를 추가/삭제하는 `type = 'categorize'` 활동이 편집 횟수에 포함되어, 실제 콘텐츠 변경이 아닌 분류 정리 작업이 트렌드 상위를 차지했습니다.
- **원인 2: 불필요한 네임스페이스(Namespace) 포함:** '사용자 토론', '미디어위키' 등 일반적인 콘텐츠 트렌드와 무관한 네임스페이스의 편집까지 모두 집계되었습니다.
- **원인 3: 사소한 편집(Minor Edits) 포함:** 오타 수정 등 `minor = TRUE`로 기록되는 사소한 편집까지 모두 집계에 포함되었습니다.

### 해결 방안: Druid SQL 쿼리에 필터 조건 추가

위 문제들을 해결하기 위해 모든 분석 쿼리의 `WHERE` 절에 다음과 같은 필터 조건을 추가하여 집계의 정확도를 높였습니다.

1.  **콘텐츠 편집 유형 필터링:** `AND "type" IN ('edit', 'new')`
    - 실제 문서의 내용이 생성되거나 수정된 활동에만 집중합니다.
2.  **핵심 네임스페이스 필터링:** `AND "namespace" IN (0, 4, 6, 14)`
    - 일반 문서(0), 프로젝트(4), 파일(6), 분류(14) 네임스페이스에만 집중합니다.
3.  **사소한 편집 제외 필터링:** `AND "minor" = FALSE`
    - 의미있는 편집 활동만을 집계 대상으로 삼습니다.

---

## 2. Q-ID의 가독성 문제 해결

노이즈를 제거한 후, 결과에 `Q`로 시작하는 위키데이터(Wikidata) 항목들이 다수 나타났습니다. 이 ID만으로는 어떤 항목에 대한 트렌드인지 파악하기 어려웠습니다.

### 문제점: Q-ID의 직관성 부족

- **원인:** 위키데이터 항목은 `Q24000605`와 같은 고유 ID로 식별됩니다. 이 ID는 기계적으로는 명확하지만 사람이 이해하기는 어렵습니다.

### 해결 방안: 도메인 정보 추가 및 Wikidata API 연동

#### 1단계: 출처 도메인 확인

- **시도:** `"meta.domain"` 컬럼을 쿼리에 추가했으나 `Column not found` 오류 발생.
- **해결:** 사용자 피드백을 통해 정확한 컬럼 이름이 `"server_name"`임을 확인하고 쿼리를 수정했습니다. 이를 통해 Q-ID가 `www.wikidata.org`에서 발생했음을 명확히 할 수 있었습니다.

#### 2단계: Q-ID에 이름(Label)과 설명(Description) 정보 통합

- **초기 해결:** `get_wikidata_label.py`라는 별도 스크립트를 작성하여, Q-ID를 입력하면 위키데이터 API를 호출해 관련 정보를 조회하도록 했습니다.
- **최종 해결:** `detect_surge.py` 스크립트에 API 조회 로직을 **통합**했습니다. Druid에서 트렌드 분석 후, 결과에 포함된 Q-ID들을 모아 **한 번의 API 호출**로 관련 정보를 가져온 뒤, 원본 데이터와 합쳐서 최종 결과를 출력하도록 개선했습니다. 이를 통해 분석과 해석이 하나의 스크립트에서 완결되도록 워크플로우를 최적화했습니다.

---

## 3. 실시간 파이프라인을 위한 Q-ID 보강 아키텍처 결정

분석 스크립트가 아닌 실시간 데이터 파이프라인에 Q-ID 보강(Enrichment) 기능을 통합하는 방안을 논의하며, 여러 아키텍처의 장단점을 비교하고 최종 방안을 결정했습니다.

### 문제점: 실시간 API 호출의 한계

- **원인 1 (지연 시간):** 외부 API를 호출하는 데 따르는 네트워크 지연은 실시간 데이터 처리의 병목이 될 수 있습니다.
- **원인 2 (호출량 제한):** 실시간으로 수많은 요청을 보내면 위키데이터 API 서버의 호출량 제한(Rate Limit)에 걸려 서비스가 차단될 위험이 있습니다. (`30분간 약 1000개의 고유 Q-ID 발생 확인`)
- **원인 3 (외부 의존성):** 위키데이터 API의 상태에 따라 우리 파이프라인 전체의 안정성이 좌우됩니다.

### 대안 비교 및 최종 결정

다양한 대안을 비교한 결과, 이 프로젝트의 요구사항(단일 컨슈머, 경량 로직, 스토리지 제약)에 가장 적합한 방식을 선택했습니다.

| 구분 | 방안 | 장점 | 단점 | 평가 |
| :--- | :--- | :--- | :--- | :--- |
| **A** | **PostgreSQL 캐시** | 인프라 재활용 | 네트워크 오버헤드, Druid DB에 부하 공유 | 차선책. 가능하지만 성능, 안정성 우려. |
| **B** | **Druid Lookups** | 아키텍처 우수, 유연성, 스토리지 효율 | **대용량 덤프 파일 처리 필수**, 온디맨드 방식 불가 | 기능적으로 강력하나, 초기 구축 복잡성이 높음. |
| **C**| **Producer-Side 온디맨드 캐싱 (SQLite)** | **덤프 불필요, 단순함, 빠름, 독립성, 영속성** | 다중 컨슈머 환경에서는 부적합 | **최종 선택.** 단일 컨슈머 환경의 제약사항에 완벽히 부합하는 가장 실용적이고 균형잡힌 방식. |

### 최종 아키텍처: SQLite를 사용한 Producer-Side 온디맨드 캐싱

- **구현 방식:**
    1.  데이터 수집기(`producer/main.py`)에 **SQLite**를 로컬 캐시 데이터베이스로 연동합니다.
    2.  Wikimedia로부터 Q-ID가 포함된 데이터를 받으면, 먼저 로컬 SQLite DB에 해당 정보가 있는지 조회합니다.
    3.  **Cache Hit (정보 있음):** API 호출 없이 DB에서 즉시 정보를 가져와 데이터에 추가(보강)합니다.
    4.  **Cache Miss (정보 없음):** 그 때만 위키데이터 API를 호출하여 정보를 가져온 뒤, **그 결과를 SQLite DB에 저장**하고 데이터를 보강합니다.
- **기대 효과:**
    - 대용량 덤프 파일이나 별도의 서버 없이 **최소한의 자원**으로 Q-ID 보강 기능을 구현합니다.
    - API 호출 횟수를 극적으로 줄여 **성능과 안정성**을 모두 확보합니다.
    - 데이터 수집 로직과 통합되어 파이프라인이 단순하게 유지됩니다.
